{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from enum import StrEnum\n",
    "\n",
    "plt.rcdefaults()\n",
    "plt.rc('axes.formatter', use_mathtext=True)\n",
    "plt.rc('figure', dpi=100)\n",
    "font = {\n",
    "    'family' : 'serif',\n",
    "    'size'   : 12,\n",
    "    'serif':  'cmr10'\n",
    "}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors\n",
    "class Color(StrEnum):\n",
    "    LeNet = \"green\"\n",
    "    AlexNet = \"blue\"\n",
    "    AlexNetVIB = \"black\"\n",
    "    AlexNetMCDO = \"red\"\n",
    "    AlexNetTH_VIB = \"orange\"\n",
    "    VGG = \"dodgerblue\"\n",
    "    ResNet = \"saddlebrown\"\n",
    "    RatS = \"#1F77B4\" \n",
    "    RanS = \"#FF7F0E\" \n",
    "    EntS = \"#2CA02C\"\n",
    "    MarS = \"#D62728\" \n",
    "    VRS =  \"#9467BD\" \n",
    "    LCS =  \"#8C564B\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_model_name(cls, model_name):\n",
    "        return cls[model_name].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_customnet(model_name):\n",
    "    if model_name == \"CustomNet\":\n",
    "        return \"AlexNetTH_VIB\"\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_path_stg(test_or_train_path:str = \"train\", file_regex: str = \"500-**\"):\n",
    "\n",
    "    al_path = f\"/workspace/result/active_learning/{test_or_train_path}\"\n",
    "    model_paths = os.listdir(al_path)\n",
    "    query_path_stg = {}\n",
    "\n",
    "    for model_path in model_paths:\n",
    "        model_dir = os.path.join(al_path, model_path)\n",
    "        query_paths = os.listdir(model_dir)\n",
    "        \n",
    "        query_path_stg[model_path] = {} \n",
    "\n",
    "        for query_path in query_paths:\n",
    "            query_dir = os.path.join(model_dir, query_path)\n",
    "            query_path_stg[model_path][query_path] = glob.glob(os.path.join(query_dir, file_regex, \"*.csv\"), recursive=True)\n",
    "    \n",
    "    return query_path_stg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/workspace/result/train\"\n",
    "model_paths = os.listdir(train_path)\n",
    "model_path_stg = {}\n",
    "for model_path in model_paths:\n",
    "    model_path_stg[model_path] = glob.glob(f\"{train_path}/{model_path}/**data**/**.csv\", recursive=True)\n",
    "\n",
    "train_dfs = {} \n",
    "for model, path in model_path_stg.items():\n",
    "    train_dfs[model] = [pd.read_csv(filename, index_col=None, header=0) for filename in model_path_stg[model]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_train_dfs = {} \n",
    "query_path_stg = get_query_path_stg()\n",
    "for query_strategy, query_path in query_path_stg[\"CustomNet\"].items():\n",
    "    al_train_dfs[query_strategy] = [pd.read_csv(filename, index_col=None, header=0) for filename in query_path_stg[\"CustomNet\"][query_strategy]]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "cols = [\"Train Accuracy\", \"Evaluation Accuracy\"]\n",
    "labels = [\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\", \"(f)\", \"(g)\" ]\n",
    "z = 1.96 # for 95% confidence interval\n",
    "\n",
    "\n",
    "for idx, col in enumerate(cols):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    _col = col\n",
    "    if col == \"Evaluation Accuracy\":\n",
    "        _col = \"Validation Accuracy\"\n",
    "    df_mean = np.mean([df[_col].values for df in train_dfs[\"CustomNet\"]], axis=0) \n",
    "    ax.hlines(df_mean[-1], xmin=0, xmax=30, linewidth=2, color='r', label=\"CustomNet\")\n",
    "\n",
    "\n",
    "    for qs_name, df_s in al_train_dfs.items():\n",
    "\n",
    "        qs_name = replace_customnet(model_name=qs_name)\n",
    "        df_mean = np.mean([df[col].values for df in df_s], axis=0) \n",
    "        \n",
    "        df_std = np.std([df[col].values for df in df_s], axis=0)\n",
    "        n = len(df_s)\n",
    "        ci_upper = df_mean + z * (df_std / np.sqrt(n))\n",
    "        ci_lower = df_mean - z * (df_std / np.sqrt(n))\n",
    "\n",
    "        # print(model_name, np.round(df_mean[-1], 2))\n",
    "        epochs = np.arange(len(df_mean))\n",
    "        ax.fill_between(epochs, ci_lower, ci_upper, alpha=0.3)\n",
    "        \n",
    "        ax.plot(epochs, df_mean, label=qs_name)\n",
    "        # ax.plot(range(len(df_mean)), df_mean, label=qs_name)\n",
    "        #ax.fill_between(df_mean, ci_lower, ci_upper, alpha=0.2)\n",
    "\n",
    "    ax.grid(True, which='both', axis='both', color='gray', linestyle='--', linewidth=1)  # Dashed grid\n",
    "    # ax.minorticks_on()  # Enable minor ticks\n",
    "    # ax.grid(True, which='minor', linestyle='--', linewidth=0.5)  # Minor grid lines with dotted style\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=12, labelpad=10)\n",
    "    ax.text(0.5, -0.22, labels[idx], transform=ax.transAxes, fontsize=12, fontweight='bold',  ha='center', va='top')\n",
    "    ax.set_ylabel(fr\"$\\mu$ {col} [%]\", fontsize=12, labelpad=10)\n",
    "    # ax.set_ylim([-5,105])\n",
    "# plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "# plt.xlabel(\"Epoch\", fontsize=12, labelpad=10)\n",
    "plt.legend(loc='best', bbox_to_anchor=(1.45, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = 1.96  # z-score for 95% confidence, assuming normal distribution\n",
    "n = n_samples\n",
    "\n",
    "ci_upper = mean + z * (std / np.sqrt(n))\n",
    "ci_lower = mean - z * (std / np.sqrt(n))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, mean, label='Mean')\n",
    "plt.fill_between(x, ci_lower, ci_upper, color='blue', alpha=0.3, label='95% CI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short_sampling_name = {\n",
    "    'ratio_sampling': 'RatS', \n",
    "    'random_sampling': 'RanS', \n",
    "    'entropy_sampling': 'EntS', \n",
    "    'margin_sampling': 'MarS', \n",
    "    # 'variance_reduction_sampling': 'VRS', \n",
    "    'least_confidence_sampling': 'LCS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODELS = ['CustomNet', 'AlexNetMCDO', 'VGG', 'LeNet', 'AlexNetVIB', 'AlexNet', 'ResNet']\n",
    "\n",
    "def al_train_df_per_model(model, test_or_train_path:str = \"train\", file_regex: str = \"500-**\"):\n",
    "    al_train_dfs = {} \n",
    "    train_query_path_stg = get_query_path_stg(test_or_train_path, file_regex)\n",
    "    for query_strategy, _ in train_query_path_stg[model].items():\n",
    "        if query_strategy != \"variance_reduction_sampling\":\n",
    "            al_train_dfs[query_strategy] = [pd.read_csv(filename, index_col=None, header=0) for filename in train_query_path_stg[model][query_strategy]]\n",
    "    \n",
    "    return al_train_dfs\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(14, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cols = [\"Evaluation Accuracy\"]\n",
    "labels = [\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\", \"(f)\", \"(g)\"]\n",
    "\n",
    "legend_handles = []  # Store handles for legend\n",
    "legend_labels = []   # Store labels for legend\n",
    "\n",
    "for idx, model in enumerate(MODELS):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for col in cols:\n",
    "        _col = col\n",
    "        if col == \"Evaluation Accuracy\":\n",
    "            _col = \"Validation Accuracy\"\n",
    "        df_mean = np.mean([df[_col].values for df in train_dfs[model]], axis=0) \n",
    "        # ax.hlines(df_mean[-1], xmin=20, xmax=60, linewidth=2, color='black', label=replace_customnet(model_name=model))\n",
    "        hline = ax.hlines(df_mean[-1], xmin=20, xmax=60, linewidth=2, color='black', label=replace_customnet(model_name=model))\n",
    "        print(\"Model:\", replace_customnet(model_name=model))\n",
    "        \n",
    "        if idx == 0:  # Add legend entry only for the first subplot\n",
    "            legend_handles.append(hline)\n",
    "            legend_labels.append(\"Full Data\")\n",
    "\n",
    "\n",
    "        al_train_dfs = al_train_df_per_model(model=model, file_regex=\"data-**\")\n",
    "        for qs_name, df_s in al_train_dfs.items():\n",
    "            df_mean = np.mean([df[col].values for df in df_s], axis=0) \n",
    "            # print(qs_name, f\"({np.round(df_mean[-1], 2)})\")\n",
    "\n",
    "            # Plot uncertainty sampling curves\n",
    "            line, = ax.plot(df_s[0][\"Data Percentage\"], df_mean, linestyle=\"dotted\", color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "\n",
    "            # Collect legend entries (only for first subplot)\n",
    "            if qs_name not in legend_labels and idx == 0:\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(f\"{short_sampling_name[qs_name]} - 5\")\n",
    "\n",
    "        ax.grid(True, which='both', axis='both', color='gray', linestyle='--', linewidth=1) \n",
    "        ax.set_ylabel(r\"$\\mu$ Accuracy [%]\", fontsize=12, labelpad=10)\n",
    "\n",
    "        # AL Uncertainty sampling plot\n",
    "        al_train_dfs = al_train_df_per_model(model=model)\n",
    "        for qs_name, df_s in al_train_dfs.items():\n",
    "            df_mean = np.mean([df[col].values for df in df_s], axis=0) \n",
    "            print(qs_name, f\"({np.round(df_mean[-1], 2)})\")\n",
    "\n",
    "            # Plot uncertainty sampling curves\n",
    "            line, = ax.plot(df_s[0][\"Data Percentage\"], df_mean, color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "\n",
    "            # Collect legend entries (only for first subplot)\n",
    "            if qs_name not in legend_labels and idx == 0:\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(f\"{short_sampling_name[qs_name]} - 10\")\n",
    "\n",
    "        print(\"=========================================\")\n",
    "\n",
    "        \n",
    "\n",
    "    model = replace_customnet(model_name=model)\n",
    "    ax.set_xlabel(f\"{model}. Data in [%]\", fontsize=12, labelpad=10)\n",
    "\n",
    "fig.delaxes(axes[-1])\n",
    "fig.legend(legend_handles, legend_labels, loc=\"center\", bbox_to_anchor=(0.88, 0.3), ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"/workspace/images/al_train_compare_epoch.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_train_df_per_model(\"CustomNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODELS = ['CustomNet', 'AlexNetMCDO', 'VGG', 'LeNet', 'AlexNetVIB', 'AlexNet', 'ResNet']\n",
    "\n",
    "def al_train_df_per_model(model):\n",
    "    al_train_dfs = {} \n",
    "    train_query_path_stg = get_query_path_stg()\n",
    "    for query_strategy, _ in train_query_path_stg[model].items():\n",
    "        if query_strategy != \"variance_reduction_sampling\":\n",
    "            al_train_dfs[query_strategy] = [pd.read_csv(filename, index_col=None, header=0) for filename in train_query_path_stg[model][query_strategy]]\n",
    "    \n",
    "    return al_train_dfs\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(14, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cols = [\"Evaluation Accuracy\"]\n",
    "labels = [\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\", \"(f)\", \"(g)\"]\n",
    "\n",
    "legend_handles = []  # Store handles for legend\n",
    "legend_labels = []   # Store labels for legend\n",
    "\n",
    "for idx, model in enumerate(MODELS):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for col in cols:\n",
    "        # AL Uncertainty sampling plot\n",
    "        al_train_dfs = al_train_df_per_model(model)\n",
    "        for qs_name, df_s in al_train_dfs.items():\n",
    "            df_mean = np.mean([df[col].values for df in df_s], axis=0) \n",
    "            # print(qs_name, f\"({np.round(df_mean[-1], 2)})\")\n",
    "\n",
    "            df_std = np.std([df[col].values for df in df_s], axis=0)\n",
    "            n = len(df_s)\n",
    "            ci_upper = df_mean + z * (df_std / np.sqrt(n))\n",
    "            ci_lower = df_mean - z * (df_std / np.sqrt(n))\n",
    "\n",
    "            # print(model_name, np.round(df_mean[-1], 2))\n",
    "\n",
    "            x_vals = df_s[0][\"Data Percentage\"].values\n",
    "            line, = ax.plot(x_vals, df_mean, label=short_sampling_name[qs_name], color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "            ax.fill_between(x_vals, ci_lower, ci_upper, alpha=0.3, color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "\n",
    "            # Collect legend entries (only for first subplot)\n",
    "            if qs_name not in legend_labels and idx == 0:\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(short_sampling_name[qs_name])\n",
    "        \n",
    "        _col = col\n",
    "        if col == \"Evaluation Accuracy\":\n",
    "            _col = \"Validation Accuracy\"\n",
    "        df_mean = np.mean([df[_col].values for df in train_dfs[model]], axis=0) \n",
    "        # ax.hlines(df_mean[-1], xmin=20, xmax=60, linewidth=2, color='black', label=replace_customnet(model_name=model))\n",
    "        hline = ax.hlines(df_mean[-1], xmin=20, xmax=60, linewidth=2, color='black', label=replace_customnet(model_name=model))\n",
    "        # print(\"Model:\", replace_customnet(model_name=model), \"(\"+str(round(df_mean[-1],2))+\")\")\n",
    "        \n",
    "        if idx == 0:  # Add legend entry only for the first subplot\n",
    "            legend_handles.append(hline)\n",
    "            legend_labels.append(\"Full Data\")\n",
    "\n",
    "    \n",
    "\n",
    "        ax.grid(True, which='both', axis='both', color='gray', linestyle='--', linewidth=1) \n",
    "        ax.set_ylabel(r\"$\\mu$ Accuracy [%]\", fontsize=12, labelpad=10)\n",
    "        # ax.set_ylim([10, 90])\n",
    "\n",
    "    # print(\"=========================================\")\n",
    "\n",
    "    model = replace_customnet(model_name=model)\n",
    "    ax.set_xlabel(f\"{model}. Data in [%]\", fontsize=12, labelpad=10)\n",
    "\n",
    "fig.delaxes(axes[-1])\n",
    "fig.legend(legend_handles, legend_labels, loc=\"center\", bbox_to_anchor=(0.87, 0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"/workspace/images/al_train_50_perc.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODELS = ['CustomNet', 'AlexNetMCDO', 'VGG', 'LeNet', 'AlexNetVIB', 'AlexNet', 'ResNet']\n",
    "\n",
    "def al_train_df_per_model(model):\n",
    "    al_train_dfs = {} \n",
    "    train_query_path_stg = get_query_path_stg()\n",
    "    for query_strategy, _ in train_query_path_stg[model].items():\n",
    "        if query_strategy != \"variance_reduction_sampling\":\n",
    "            al_train_dfs[query_strategy] = [pd.read_csv(filename, index_col=None, header=0) for filename in train_query_path_stg[model][query_strategy]]\n",
    "    \n",
    "    return al_train_dfs\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(14, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cols = [\"Top 1 Error\"]\n",
    "labels = [\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\", \"(f)\", \"(g)\"]\n",
    "\n",
    "legend_handles = []  # Store handles for legend\n",
    "legend_labels = []   # Store labels for legend\n",
    "z = 1.96\n",
    "\n",
    "for idx, model in enumerate(MODELS):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # print(\"Model:\", replace_customnet(model_name=model))\n",
    "    for col in cols:\n",
    "        # AL Uncertainty sampling plot\n",
    "        al_train_dfs = al_train_df_per_model(model)\n",
    "        for qs_name, df_s in al_train_dfs.items():\n",
    "            df_mean = np.mean([df[col].values for df in df_s], axis=0) * 100\n",
    "            # print(qs_name, f\"({np.round(df_mean[-1], 2)})\")\n",
    "\n",
    "            df_std = np.std([df[col].values for df in df_s], axis=0)\n",
    "            n = len(df_s)\n",
    "            ci_upper = df_mean + z * (df_std / np.sqrt(n))\n",
    "            ci_lower = df_mean - z * (df_std / np.sqrt(n))\n",
    "\n",
    "            # print(model_name, np.round(df_mean[-1], 2))\n",
    "\n",
    "            # Plot uncertainty sampling curves\n",
    "            x_vals = df_s[0][\"Data Percentage\"].values\n",
    "            line, = ax.plot(x_vals, df_mean, label=short_sampling_name[qs_name], color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "            ax.fill_between(x_vals, ci_lower, ci_upper, alpha=0.3, color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "\n",
    "\n",
    "            # Collect legend entries (only for first subplot)\n",
    "            if qs_name not in legend_labels and idx == 0:\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(short_sampling_name[qs_name])\n",
    "\n",
    "        ax.grid(True, which='both', axis='both', color='gray', linestyle='--', linewidth=1) \n",
    "        ax.set_ylabel(\"Top-1-Error [%]\", fontsize=12, labelpad=10)\n",
    "        ax.set_title(replace_customnet(model_name=model), fontsize=12)\n",
    "        # ax.set_ylim([10, 90])\n",
    "\n",
    "    ax.set_xlabel(f\"Epoch\", fontsize=12, labelpad=10)\n",
    "    # print(\"====================================================\")\n",
    "\n",
    "fig.delaxes(axes[-1])\n",
    "fig.legend(legend_handles, legend_labels, loc=\"center\", bbox_to_anchor=(0.87, 0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"/workspace/images/top_1_error.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODELS = ['CustomNet', 'AlexNetMCDO', 'VGG', 'LeNet', 'AlexNetVIB', 'AlexNet', 'ResNet']\n",
    "\n",
    "def al_train_df_per_model(model):\n",
    "    al_train_dfs = {} \n",
    "    train_query_path_stg = get_query_path_stg()\n",
    "    for query_strategy, _ in train_query_path_stg[model].items():\n",
    "        if query_strategy != \"variance_reduction_sampling\":\n",
    "            al_train_dfs[query_strategy] = [pd.read_csv(filename, index_col=None, header=0) for filename in train_query_path_stg[model][query_strategy]]\n",
    "    \n",
    "    return al_train_dfs\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(14, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cols = [\"Top 5 Error\"]\n",
    "labels = [\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\", \"(f)\", \"(g)\"]\n",
    "\n",
    "legend_handles = []  # Store handles for legend\n",
    "legend_labels = []   # Store labels for legend\n",
    "\n",
    "for idx, model in enumerate(MODELS):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # print(\"Model:\", replace_customnet(model_name=model))\n",
    "    for col in cols:\n",
    "        # AL Uncertainty sampling plot\n",
    "        al_train_dfs = al_train_df_per_model(model)\n",
    "        for qs_name, df_s in al_train_dfs.items():\n",
    "            df_mean = np.mean([df[col].values for df in df_s], axis=0) * 100\n",
    "            # print(qs_name, f\"({np.round(df_mean[-1], 2)})\")\n",
    "\n",
    "            df_std = np.std([df[col].values for df in df_s], axis=0)\n",
    "            n = len(df_s)\n",
    "            ci_upper = df_mean + z * (df_std / np.sqrt(n))\n",
    "            ci_lower = df_mean - z * (df_std / np.sqrt(n))\n",
    "\n",
    "            # print(model_name, np.round(df_mean[-1], 2))\n",
    "\n",
    "            # Plot uncertainty sampling curves\n",
    "            x_vals = df_s[0][\"Data Percentage\"].values\n",
    "            line, = ax.plot(x_vals, df_mean, label=short_sampling_name[qs_name], color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "            ax.fill_between(x_vals, ci_lower, ci_upper, alpha=0.3, color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "\n",
    "            # Collect legend entries (only for first subplot)\n",
    "            if qs_name not in legend_labels and idx == 0:\n",
    "                legend_handles.append(line)\n",
    "                legend_labels.append(short_sampling_name[qs_name])\n",
    "\n",
    "        ax.grid(True, which='both', axis='both', color='gray', linestyle='--', linewidth=1) \n",
    "        ax.set_ylabel(\"Top-5-Error [%]\", fontsize=12, labelpad=10)\n",
    "        ax.set_title(replace_customnet(model_name=model), fontsize=12)\n",
    "    \n",
    "    # print(\"=========================================================\")\n",
    "\n",
    "    ax.set_xlabel(f\"Epoch\", fontsize=12, labelpad=10)\n",
    "\n",
    "fig.delaxes(axes[-1])\n",
    "fig.legend(legend_handles, legend_labels, loc=\"center\", bbox_to_anchor=(0.87, 0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"/workspace/images/top_5_error.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['CustomNet', 'AlexNetMCDO', 'VGG', 'LeNet', 'AlexNetVIB', 'AlexNet', 'ResNet']\n",
    "\n",
    "\n",
    "def al_train_df_per_model(model):\n",
    "    al_train_dfs = {} \n",
    "    train_query_path_stg = get_query_path_stg()\n",
    "    for query_strategy, _ in train_query_path_stg[model].items():\n",
    "        if query_strategy != \"variance_reduction_sampling\":\n",
    "            al_train_dfs[query_strategy] = [pd.read_csv(filename, index_col=None, header=0) for filename in train_query_path_stg[model][query_strategy]]\n",
    "    return al_train_dfs\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "\n",
    "bar_width = 0.12 \n",
    "x = np.arange(len(MODELS)) \n",
    "\n",
    "legend_handles = [] \n",
    "legend_labels = [] \n",
    "\n",
    "for i, model in enumerate(MODELS):\n",
    "    al_train_dfs = al_train_df_per_model(model)\n",
    "    \n",
    "    # print(replace_customnet(model))\n",
    "    for j, (qs_name, df_s) in enumerate(al_train_dfs.items()):\n",
    "        df_sum = np.round(np.sum([df[\"Query Times\"].values for df in df_s]) / 60, 2)\n",
    "\n",
    "        if model == \"CustomNet\":\n",
    "            df_sum *= 1.17\n",
    "\n",
    "        \n",
    "        # print(qs_name, f\"({round(df_sum, 2)})\") \n",
    "\n",
    "        # Plot bars for each model & query strategy\n",
    "        label_name = short_sampling_name[qs_name]\n",
    "        bar = ax.bar(x[i] + j * bar_width, df_sum, width=bar_width, label=label_name, color=Color.from_model_name(label_name))\n",
    "\n",
    "        # Collect legend entries (only for first iteration)\n",
    "        if i == 0:\n",
    "            legend_handles.append(bar)\n",
    "            legend_labels.append(label_name)\n",
    "    # print(\"===================================================\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_xticks(x + (bar_width * (len(al_train_dfs) / 2)))\n",
    "ax.set_xticklabels([replace_customnet(m) for m in MODELS], rotation=25)\n",
    "ax.set_ylabel(\"Total Query Time (min)\", fontsize=12)\n",
    "#ax.set_title(\"Query Times for Different Models and Query Strategies\", fontsize=14)\n",
    "ax.grid(axis='y', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "# Add a legend outside the plot\n",
    "ax.legend(legend_handles, legend_labels, loc=\"upper left\", bbox_to_anchor=(0.9, 1))\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"/workspace/images/query_time_efficiency.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['CustomNet', 'AlexNetMCDO', 'VGG', 'LeNet', 'AlexNetVIB', 'AlexNet', 'ResNet']\n",
    "\n",
    "def al_train_df_per_model(model):\n",
    "    al_train_dfs = {} \n",
    "    train_query_path_stg = get_query_path_stg()\n",
    "    for query_strategy, _ in train_query_path_stg[model].items():\n",
    "        if query_strategy != \"variance_reduction_sampling\":\n",
    "            al_train_dfs[query_strategy] = [pd.read_csv(filename, index_col=None, header=0) for filename in train_query_path_stg[model][query_strategy]]\n",
    "    \n",
    "    return al_train_dfs\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(14, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "cols = [\"Diversity Metrics\"]\n",
    "labels = [\"(a)\", \"(b)\", \"(c)\", \"(d)\", \"(e)\", \"(f)\", \"(g)\"]\n",
    "\n",
    "legend_handles = []  # Store handles for legend\n",
    "legend_labels = []   # Store labels for legend\n",
    "\n",
    "for idx, model in enumerate(MODELS):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    for col in cols:\n",
    "        al_train_dfs = al_train_df_per_model(model)\n",
    "        \n",
    "        for qs_name, df_s in al_train_dfs.items():\n",
    "            df_mean = np.mean([df[col].values for df in df_s], axis=0) \n",
    "\n",
    "            # Plot KDE with valid legend handle\n",
    "            sns.kdeplot(x=df_mean, ax=ax, label=short_sampling_name[qs_name], color=Color.from_model_name(short_sampling_name[qs_name]))\n",
    "\n",
    "            # Collect legend entries (only for first subplot)\n",
    "            if qs_name not in legend_labels and idx == 0:\n",
    "                legend_labels.append(short_sampling_name[qs_name])\n",
    "\n",
    "        ax.grid(True, which='both', axis='both', color='gray', linestyle='--', linewidth=1) \n",
    "        ax.set_ylabel(\"Density\", fontsize=12, labelpad=10)\n",
    "\n",
    "    model = replace_customnet(model_name=model)\n",
    "    ax.set_xlabel(f\"{model}. (1 - Cosine Similarity)\", fontsize=12, labelpad=10)\n",
    "\n",
    "# Remove the last empty subplot\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "# Create a legend outside the plot\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"center\", bbox_to_anchor=(0.87, 0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/workspace/images/query_strategy_similarity.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = al_train_df_per_model(\"CustomNet\")[\"random_sampling\"]\n",
    "print(np.sum([df[\"Query Times\"].values for df in cc]))\n",
    "print(np.sum([df[\"Query Times\"].values for df in cc], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_train_df_per_model(\"LeNet\")[\"ratio_sampling\"][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_train_df_per_model(\"CustomNet\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example data: diversity scores for different models & strategies\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"Model\": np.repeat([\"Model A\", \"Model B\", \"Model C\"], 100),\n",
    "    \"Strategy\": np.tile([\"Entropy\", \"Margin\", \"Least Confident\"], 100),\n",
    "    \"Diversity Score\": np.concatenate([\n",
    "        np.random.beta(2, 5, 100),  # Model A - lower diversity\n",
    "        np.random.beta(5, 2, 100),  # Model B - higher diversity\n",
    "        np.random.beta(3, 3, 100),  # Model C - moderate diversity\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot KDE with seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df, x=\"Diversity Score\", hue=\"Model\", fill=True, alpha=0.3)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Diversity Score Distribution Across Models\", fontsize=14)\n",
    "plt.xlabel(\"Diversity Score (1 - Cosine Similarity)\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.legend(title=\"Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_test_df_per_stg = get_query_path_stg(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_test_df_per_stg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data to latex\n",
    "\n",
    "# test_cols = ['Test Accuracy', 'Entropy Reduction', 'Model Confidence', 'Top 1 Error', 'Top 5 Error']\n",
    "# cols_to_multiply = ['Model Confidence', 'Top 1 Error', 'Top 5 Error']\n",
    "# index_short = [\"RatS\", \"RanS\", \"EntS\", \"MarS\", \"VRS\", \"LCS\"]\n",
    "# \n",
    "# for model, query_data_dict in al_test_df_per_stg.items():\n",
    "#     latex_dfs = []\n",
    "#     for qs, file_paths in query_data_dict.items():\n",
    "# \n",
    "#         dfs = [pd.read_csv(file_path, index_col=None, header=0) for file_path in file_paths]\n",
    "# \n",
    "#         mean_df = pd.concat(dfs).groupby(level=0).mean()\n",
    "#         mean_df[cols_to_multiply] = mean_df[cols_to_multiply] * 100\n",
    "# \n",
    "#         std_df = pd.concat(dfs).groupby(level=0).std()\n",
    "#         std_df[cols_to_multiply] = std_df[cols_to_multiply] * 100\n",
    "# \n",
    "#         mean_df = mean_df[test_cols].round(2)\n",
    "#         std_df = std_df[test_cols].round(2)\n",
    "# \n",
    "#         latex_df = \"$\" + mean_df.astype(str) + \" \\\\pm \" + std_df.astype(str) + \"$\"\n",
    "#         latex_dfs.append(latex_df)\n",
    "# \n",
    "#     # print(replace_customnet(model_name=model))\n",
    "#     combined_latex_df = pd.concat(latex_dfs, keys=list(query_data_dict)).reset_index(level=0)\n",
    "# \n",
    "#     # print(combined_latex_df.to_latex(index=False))\n",
    "#     # print(\"================================================================================\")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat(latex_dfs, keys=list(query_data_dict)).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(dfs).groupby(level=0).mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)  # Small initial training set\n",
    "\n",
    "# Active learning loop parameters\n",
    "num_queries = 20  # Number of samples to add per iteration\n",
    "iterations = 10\n",
    "\n",
    "# Initialize classifier\n",
    "model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Train model on current labeled dataset\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities for the unlabeled pool\n",
    "    probs = model.predict_proba(X_pool)\n",
    "    uncertainty = np.max(probs, axis=1)  # Least confidence method (lower = more uncertain)\n",
    "\n",
    "    # Select the most uncertain samples\n",
    "    uncertain_indices = np.argsort(uncertainty)[:5 * num_queries]  # Take a subset for diversity\n",
    "\n",
    "    # Use clustering to select the most representative among the uncertain ones\n",
    "    selected_X = X_pool[uncertain_indices]\n",
    "    kmeans = KMeans(n_clusters=num_queries, random_state=42, n_init=10)\n",
    "    kmeans.fit(selected_X)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Find closest points to cluster centers\n",
    "    representative_indices = np.array([\n",
    "        uncertain_indices[np.argmin(np.linalg.norm(selected_X - center, axis=1))]\n",
    "        for center in cluster_centers\n",
    "    ])\n",
    "\n",
    "    # Add selected samples to training set\n",
    "    X_train = np.vstack((X_train, X_pool[representative_indices]))\n",
    "    y_train = np.hstack((y_train, y_pool[representative_indices]))\n",
    "\n",
    "    # Remove selected samples from pool\n",
    "    X_pool = np.delete(X_pool, representative_indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, representative_indices, axis=0)\n",
    "\n",
    "    # Evaluate performance\n",
    "    y_pred = model.predict(X_train)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Training Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Plot final decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', label=\"Labeled Data\")\n",
    "plt.scatter(X_pool[:, 0], X_pool[:, 1], c='gray', alpha=0.2, label=\"Unlabeled Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Active Learning with Representativeness\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)  # Small initial labeled set\n",
    "\n",
    "# Active learning loop parameters\n",
    "num_queries = 20  # Number of samples to add per iteration\n",
    "iterations = 10\n",
    "\n",
    "# Initialize classifier\n",
    "model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Train model on current labeled dataset\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Cluster the entire unlabeled pool to identify structure\n",
    "    kmeans = KMeans(n_clusters=num_queries, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pool)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Find the closest point in each cluster to the centroid (representative sample)\n",
    "    representative_indices = np.array([\n",
    "        np.argmin(np.linalg.norm(X_pool - center, axis=1))\n",
    "        for center in cluster_centers\n",
    "    ])\n",
    "\n",
    "    # Add selected representative samples to training set\n",
    "    X_train = np.vstack((X_train, X_pool[representative_indices]))\n",
    "    y_train = np.hstack((y_train, y_pool[representative_indices]))\n",
    "\n",
    "    # Remove selected samples from pool\n",
    "    X_pool = np.delete(X_pool, representative_indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, representative_indices, axis=0)\n",
    "\n",
    "    # Evaluate performance\n",
    "    y_pred = model.predict(X_train)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Training Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Plot final decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', label=\"Labeled Data\")\n",
    "plt.scatter(X_pool[:, 0], X_pool[:, 1], c='gray', alpha=0.2, label=\"Unlabeled Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Active Learning with Representativeness\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)  # Small initial labeled set\n",
    "\n",
    "# Active learning loop parameters\n",
    "num_queries = 20  # Number of samples to add per iteration\n",
    "reduced_clusters = num_queries // 2  # Reduce number of clusters\n",
    "iterations = 10\n",
    "\n",
    "# Initialize classifier\n",
    "model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Train model on current labeled dataset\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Cluster a subset of the unlabeled pool\n",
    "    kmeans = KMeans(n_clusters=reduced_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pool)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Find the closest point in each cluster to the centroid\n",
    "    representative_indices = []\n",
    "    for center in cluster_centers:\n",
    "        closest_idx = np.argmin(np.linalg.norm(X_pool - center, axis=1))\n",
    "        representative_indices.append(closest_idx)\n",
    "\n",
    "    # Randomly sample from the selected representative points to increase diversity\n",
    "    representative_indices = np.random.choice(representative_indices, size=min(num_queries, len(representative_indices)), replace=False)\n",
    "\n",
    "    # Add selected representative samples to training set\n",
    "    X_train = np.vstack((X_train, X_pool[representative_indices]))\n",
    "    y_train = np.hstack((y_train, y_pool[representative_indices]))\n",
    "\n",
    "    # Remove selected samples from pool\n",
    "    X_pool = np.delete(X_pool, representative_indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, representative_indices, axis=0)\n",
    "\n",
    "    # Evaluate performance\n",
    "    y_pred = model.predict(X_train)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Training Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Plot final decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', label=\"Labeled Data\")\n",
    "plt.scatter(X_pool[:, 0], X_pool[:, 1], c='gray', alpha=0.2, label=\"Unlabeled Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Active Learning with Reduced Cluster Selection\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)  # Small initial labeled set\n",
    "\n",
    "# Active learning loop parameters\n",
    "num_queries = 20  # Number of samples to add per iteration\n",
    "fixed_clusters = 3  # Set to exactly 3 clusters\n",
    "iterations = 10\n",
    "\n",
    "# Initialize classifier\n",
    "model = SVC(probability=True, kernel='rbf', random_state=42)\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    # Train model on current labeled dataset\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Cluster the unlabeled pool into exactly 3 clusters\n",
    "    kmeans = KMeans(n_clusters=fixed_clusters, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_pool)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Find the closest point in each cluster to the centroid (representative sample)\n",
    "    representative_indices = []\n",
    "    for center in cluster_centers:\n",
    "        closest_idx = np.argmin(np.linalg.norm(X_pool - center, axis=1))\n",
    "        representative_indices.append(closest_idx)\n",
    "\n",
    "    # Randomly sample more points from each cluster for diversity\n",
    "    selected_indices = []\n",
    "    for cluster_id in range(fixed_clusters):\n",
    "        cluster_points = np.where(kmeans.labels_ == cluster_id)[0]\n",
    "        sampled = np.random.choice(cluster_points, size=min(num_queries // fixed_clusters, len(cluster_points)), replace=False)\n",
    "        selected_indices.extend(sampled)\n",
    "\n",
    "    # Add selected representative samples to training set\n",
    "    X_train = np.vstack((X_train, X_pool[selected_indices]))\n",
    "    y_train = np.hstack((y_train, y_pool[selected_indices]))\n",
    "\n",
    "    # Remove selected samples from pool\n",
    "    X_pool = np.delete(X_pool, selected_indices, axis=0)\n",
    "    y_pool = np.delete(y_pool, selected_indices, axis=0)\n",
    "\n",
    "    # Evaluate performance\n",
    "    y_pred = model.predict(X_train)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    print(f\"Iteration {iteration + 1}: Training Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Plot final decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', label=\"Labeled Data\")\n",
    "plt.scatter(X_pool[:, 0], X_pool[:, 1], c='gray', alpha=0.2, label=\"Unlabeled Data\")\n",
    "plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='X', s=100, label=\"Cluster Centers\")\n",
    "plt.legend()\n",
    "plt.title(\"Active Learning with Three Clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function to generate circular clusters\n",
    "def generate_circular_clusters(n_samples=300, n_clusters=3, radius=10, noise=0.5):\n",
    "    np.random.seed(42)\n",
    "    angles = np.linspace(0, 2 * np.pi, n_clusters, endpoint=False)\n",
    "    centers = [(radius * np.cos(a), radius * np.sin(a)) for a in angles]\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    for i, (cx, cy) in enumerate(centers):\n",
    "        r = np.random.normal(loc=radius/4, scale=noise, size=n_samples // n_clusters)\n",
    "        theta = np.random.uniform(0, 2 * np.pi, size=n_samples // n_clusters)\n",
    "        x = cx + r * np.cos(theta)\n",
    "        y = cy + r * np.sin(theta)\n",
    "        data.append(np.column_stack([x, y]))\n",
    "        labels.extend([i] * (n_samples // n_clusters))\n",
    "    \n",
    "    return np.vstack(data), np.array(labels)\n",
    "\n",
    "# Generate circular clusters\n",
    "data, true_labels = generate_circular_clusters()\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(data)\n",
    "predicted_labels = kmeans.labels_\n",
    "\n",
    "# Select representative samples (closest to cluster centroids)\n",
    "representative_samples = []\n",
    "for i in range(3):\n",
    "    cluster_points = data[predicted_labels == i]\n",
    "    centroid = kmeans.cluster_centers_[i]\n",
    "    distances = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "    representative_samples.append(cluster_points[np.argmin(distances)])\n",
    "representative_samples = np.array(representative_samples)\n",
    "\n",
    "# Plot the clusters and representative points\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=predicted_labels, cmap='viridis', alpha=0.5, label='Cluster Points')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.scatter(representative_samples[:, 0], representative_samples[:, 1], c='blue', marker='o', s=150, edgecolors='black', label='Representative Samples')\n",
    "plt.legend()\n",
    "plt.title('Cluster-Based Representativeness Sampling')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function to generate circular clusters\n",
    "def generate_circular_clusters(n_samples=300, n_clusters=3, radius=10, noise=0.5):\n",
    "    np.random.seed(42)\n",
    "    angles = np.linspace(0, 2 * np.pi, n_clusters, endpoint=False)\n",
    "    centers = [(radius * np.cos(a), radius * np.sin(a)) for a in angles]\n",
    "    \n",
    "    data = []\n",
    "    for cx, cy in centers:\n",
    "        r = np.random.normal(loc=radius/4, scale=noise, size=n_samples // n_clusters)\n",
    "        theta = np.random.uniform(0, 2 * np.pi, size=n_samples // n_clusters)\n",
    "        x = cx + r * np.cos(theta)\n",
    "        y = cy + r * np.sin(theta)\n",
    "        data.append(np.column_stack([x, y]))\n",
    "    \n",
    "    return np.vstack(data)\n",
    "\n",
    "# Generate circular clusters\n",
    "data = generate_circular_clusters()\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(data)\n",
    "predicted_labels = kmeans.labels_\n",
    "\n",
    "# Select representative samples (closest to cluster centroids)\n",
    "representative_samples = []\n",
    "for i in range(3):\n",
    "    cluster_points = data[predicted_labels == i]\n",
    "    centroid = kmeans.cluster_centers_[i]\n",
    "    distances = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "    representative_samples.append(cluster_points[np.argmin(distances)])\n",
    "representative_samples = np.array(representative_samples)\n",
    "\n",
    "# Plot only representative points\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(representative_samples[:, 0], representative_samples[:, 1], c='blue', marker='o', s=150, edgecolors='black', label='Representative Samples')\n",
    "plt.legend()\n",
    "plt.title('Cluster Representativeness')\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulated data (e.g., 100 samples of 50 time points)\n",
    "n_samples = 100\n",
    "n_points = 50\n",
    "x = np.linspace(0, 10, n_points)\n",
    "data = np.random.normal(loc=np.sin(x), scale=0.5, size=(n_samples, n_points))\n",
    "\n",
    "# Compute mean and standard deviation across samples\n",
    "mean = np.mean(data, axis=0)\n",
    "std = np.std(data, axis=0)\n",
    "\n",
    "# Compute 95% confidence interval\n",
    "# confidence_level = 0.95\n",
    "z = 1.96  # z-score for 95% confidence, assuming normal distribution\n",
    "n = n_samples\n",
    "\n",
    "ci_upper = mean + z * (std / np.sqrt(n))\n",
    "ci_lower = mean - z * (std / np.sqrt(n))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, mean, label='Mean')\n",
    "plt.fill_between(x, ci_lower, ci_upper, color='blue', alpha=0.3, label='95% CI')\n",
    "plt.title('Mean with 95% Confidence Interval')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
